{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certificate in Data Science | Assignment 10 |  \n",
    "> University of Washington, Seattle, WA    \n",
    "> January 2020  \n",
    "> N. Hicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next generation search engine startup was successful in having the ability to search for images based on their content. As a result, the startup received its second round of funding to be able to search news articles based on their topic. As the lead data scientist, you are tasked to ***build a model that classifies the topic of each article or newswire***.\n",
    "\n",
    "- Leverage the RNN_KERAS.ipynb lab in the lesson.\n",
    "\n",
    "- Use the Keras Reuters newswire topics classification dataset.\n",
    "\n",
    "This dataset contains 11,228 newswires from Reuters, labeled with over 46 topics. Each wire is encoded as a sequence of word indexes. For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\". As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Keras dataset, create a new notebook and perform each of the following data preparation tasks and answer the related questions:\n",
    "    - Read the Reuters dataset into both training and testing datasets. \n",
    "    - Prepare the dataset for modeling.\n",
    "    - Build/compile 3 different models using Keras LTSM ideally to improve the model each iteration.\n",
    "    - Describe and explain your findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:17.499673Z",
     "start_time": "2020-01-05T01:46:08.654144Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import reuters\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:17.575274Z",
     "start_time": "2020-01-05T01:46:17.499673Z"
    },
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Report the Numpy n-dimensional array characteristics.\n",
    "RETURN: None; print the characteristics.\n",
    "'''\n",
    "def print_array_attrs(arr, txt):\n",
    "    print('--------------------------')\n",
    "    print('DATASET                 {}'.format(txt))\n",
    "    print('dType                   {}'.format(arr.dtype))      # the array data type\n",
    "    print('num_dimensions          {}'.format(arr.ndim))       # numbr of dimensions\n",
    "    print('shape                   {}'.format(arr.shape))      # the array shape\n",
    "    print('stride                  {}'.format(arr.strides))    # the stride of the array\n",
    "    print('total num_elements      {}\\n'.format(arr.size))       # number of elements\n",
    "    print('memory address          {}'.format(arr.data))       # the memory address\n",
    "    print('element length, bytes   {}'.format(arr.itemsize))   # length of one array element, in bytes\n",
    "    print('elements size, bytes    {}'.format(arr.nbytes))     # total bytes consumed of the elements\n",
    "    print('memory layout\\n{}'.format(arr.flags))      # memory layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:17.617775Z",
     "start_time": "2020-01-05T01:46:17.597775Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Decode the mapped integers to the assigned words\n",
    "'''\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:17.645275Z",
     "start_time": "2020-01-05T01:46:17.620275Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Create a scale function for a single feature.\n",
    "INPUT: pd.Series; a single attribute.\n",
    "RETURN: a scaled column attribute.\n",
    "'''\n",
    "def scale(col):\n",
    "    mean_col = np.mean(col)\n",
    "    sd_col = np.std(col)\n",
    "    std = (col - mean_col) / sd_col\n",
    "    return std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:18.978980Z",
     "start_time": "2020-01-05T01:46:17.647776Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the dataset from Keras\n",
    "num_of_words=10000\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\n",
    "                                                         num_words=num_of_words,\n",
    "                                                         skip_top=0,\n",
    "                                                         maxlen=None,\n",
    "                                                         test_split=0.2,\n",
    "                                                         seed=113,\n",
    "                                                         start_char=1,\n",
    "                                                         oov_char=2,\n",
    "                                                         index_from=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:19.010180Z",
     "start_time": "2020-01-05T01:46:18.978980Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "DATASET                 x_train\n",
      "dType                   object\n",
      "num_dimensions          1\n",
      "shape                   (8982,)\n",
      "stride                  (8,)\n",
      "total num_elements      8982\n",
      "\n",
      "memory address          <memory at 0x000000001001EB88>\n",
      "element length, bytes   8\n",
      "elements size, bytes    71856\n",
      "memory layout\n",
      "  C_CONTIGUOUS : True\n",
      "  F_CONTIGUOUS : True\n",
      "  OWNDATA : True\n",
      "  WRITEABLE : True\n",
      "  ALIGNED : True\n",
      "  WRITEBACKIFCOPY : False\n",
      "  UPDATEIFCOPY : False\n",
      "\n",
      "--------------------------\n",
      "DATASET                 y_train\n",
      "dType                   int64\n",
      "num_dimensions          1\n",
      "shape                   (8982,)\n",
      "stride                  (8,)\n",
      "total num_elements      8982\n",
      "\n",
      "memory address          <memory at 0x000000001001EB88>\n",
      "element length, bytes   8\n",
      "elements size, bytes    71856\n",
      "memory layout\n",
      "  C_CONTIGUOUS : True\n",
      "  F_CONTIGUOUS : True\n",
      "  OWNDATA : True\n",
      "  WRITEABLE : True\n",
      "  ALIGNED : True\n",
      "  WRITEBACKIFCOPY : False\n",
      "  UPDATEIFCOPY : False\n",
      "\n",
      "--------------------------\n",
      "DATASET                 x_test\n",
      "dType                   object\n",
      "num_dimensions          1\n",
      "shape                   (2246,)\n",
      "stride                  (8,)\n",
      "total num_elements      2246\n",
      "\n",
      "memory address          <memory at 0x000000001001EB88>\n",
      "element length, bytes   8\n",
      "elements size, bytes    17968\n",
      "memory layout\n",
      "  C_CONTIGUOUS : True\n",
      "  F_CONTIGUOUS : True\n",
      "  OWNDATA : True\n",
      "  WRITEABLE : True\n",
      "  ALIGNED : True\n",
      "  WRITEBACKIFCOPY : False\n",
      "  UPDATEIFCOPY : False\n",
      "\n",
      "--------------------------\n",
      "DATASET                 y_test\n",
      "dType                   int64\n",
      "num_dimensions          1\n",
      "shape                   (2246,)\n",
      "stride                  (8,)\n",
      "total num_elements      2246\n",
      "\n",
      "memory address          <memory at 0x000000001001EB88>\n",
      "element length, bytes   8\n",
      "elements size, bytes    17968\n",
      "memory layout\n",
      "  C_CONTIGUOUS : True\n",
      "  F_CONTIGUOUS : True\n",
      "  OWNDATA : True\n",
      "  WRITEABLE : True\n",
      "  ALIGNED : True\n",
      "  WRITEBACKIFCOPY : False\n",
      "  UPDATEIFCOPY : False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# return the memory address of each array\n",
    "arrays = [x_train, y_train, x_test, y_test]\n",
    "labels = ['x_train', 'y_train', 'x_test', 'y_test']\n",
    "for (arr, txt) in zip(arrays, labels):\n",
    "    print_array_attrs(arr, txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:19.041380Z",
     "start_time": "2020-01-05T01:46:19.010180Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:19.061380Z",
     "start_time": "2020-01-05T01:46:19.046380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 1378, 2025, 9, 697, 4622, 111, 8, 25, 109, 29, 3650, 11, 150, 244, 364, 33, 30, 30, 1398, 333, 6, 2, 159, 9, 1084, 363, 13, 2, 71, 9, 2, 71, 117, 4, 225, 78, 206, 10, 9, 1214, 8, 4, 270, 5, 2, 7, 748, 48, 9, 2, 7, 207, 1451, 966, 1864, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 3465, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 1214, 855, 129, 783, 21, 4, 2280, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 1214, 23, 644, 47, 20, 324, 27, 56, 2, 2, 5, 192, 510, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the Integers to Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:19.363184Z",
     "start_time": "2020-01-05T01:46:19.061380Z"
    }
   },
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode the Mapped Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:19.393185Z",
     "start_time": "2020-01-05T01:46:19.365684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> <UNK> <UNK> said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review the words\n",
    "decode_review(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Models - Keras LTSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:19.423185Z",
     "start_time": "2020-01-05T01:46:19.405685Z"
    }
   },
   "outputs": [],
   "source": [
    "# scale the feature arrays\n",
    "x_train[0] = scale(x_train[0])\n",
    "x_test[0] = scale(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:19.968093Z",
     "start_time": "2020-01-05T01:46:19.425685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Only consider the first 400 words within the review\n",
    "max_review_length = 400\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM(units=100, activation='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T01:46:21.174295Z",
     "start_time": "2020-01-05T01:46:19.968093Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Construct our model\n",
    "embedding_vecor_length = 32\n",
    "model_0 = keras.models.Sequential()\n",
    "model_0.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "# basic default LSTM model\n",
    "model_0.add(keras.layers.LSTM(units=100, activation='tanh'))\n",
    "model_0.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model_0.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T02:45:41.935774Z",
     "start_time": "2020-01-05T01:46:21.174295Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 400, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 373,301\n",
      "Trainable params: 373,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/3\n",
      "8982/8982 [==============================] - 1236s 138ms/sample - loss: -131.5785 - accuracy: 0.0481 - val_loss: -208.2405 - val_accuracy: 0.0467\n",
      "Epoch 2/3\n",
      "8982/8982 [==============================] - 1154s 129ms/sample - loss: -264.8070 - accuracy: 0.0481 - val_loss: -323.3691 - val_accuracy: 0.0467\n",
      "Epoch 3/3\n",
      "8982/8982 [==============================] - 1170s 130ms/sample - loss: -377.3464 - accuracy: 0.0481 - val_loss: -434.9604 - val_accuracy: 0.0467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x119f3a08>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_0.summary())\n",
    "x_train_0, y_train_0 = x_train.copy(), y_train.copy()\n",
    "model_0.fit(x_train_0, y_train_0, validation_data=(x_test, y_test), epochs=3, batch_size=64)   # ensure the batch size divides the feature array size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T02:47:46.173911Z",
     "start_time": "2020-01-05T02:45:41.958275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 4.674978%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model_0.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.6f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM(units=100, activation='sigmoid', dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T02:47:47.045520Z",
     "start_time": "2020-01-05T02:47:46.176411Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct our model\n",
    "embedding_vecor_length = 32\n",
    "model_1 = keras.models.Sequential()\n",
    "model_1.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "# update the model by adding hyperpareter dropout=0.2\n",
    "model_1.add(keras.layers.LSTM(units=100, activation='sigmoid', dropout=0.2))\n",
    "model_1.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T03:50:19.334367Z",
     "start_time": "2020-01-05T02:47:47.045520Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 373,301\n",
      "Trainable params: 373,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/3\n",
      "8982/8982 [==============================] - 1237s 138ms/sample - loss: -62.1062 - accuracy: 0.0468 - val_loss: -110.0967 - val_accuracy: 0.0467\n",
      "Epoch 2/3\n",
      "8982/8982 [==============================] - 1271s 141ms/sample - loss: -149.2300 - accuracy: 0.0481 - val_loss: -189.1818 - val_accuracy: 0.0467\n",
      "Epoch 3/3\n",
      "8982/8982 [==============================] - 1244s 139ms/sample - loss: -225.2734 - accuracy: 0.0481 - val_loss: -263.5471 - val_accuracy: 0.0467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b4dbe88>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_1.summary())\n",
    "x_train_1, y_train_1 = x_train.copy(), y_train.copy()\n",
    "model_1.fit(x_train_1, y_train_1, validation_data=(x_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T03:52:48.519130Z",
     "start_time": "2020-01-05T03:50:19.397468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 4.674978%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model_1.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.6f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM(units=100, activation='sigmoid', dropout=0.2, use_bias=True, unit_forget_bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_2.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=6, batch_size=138)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T03:52:49.770534Z",
     "start_time": "2020-01-05T03:52:48.519130Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct the model\n",
    "embedding_vecor_length = 32\n",
    "model_2 = keras.models.Sequential()\n",
    "model_2.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "# add new LSTM hyperparameter: us_bias=True and unit_forget_bias=True\n",
    "model_2.add(keras.layers.LSTM(units=100, activation='sigmoid', dropout=0.2, use_bias=True, unit_forget_bias=True))\n",
    "model_2.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T05:11:07.947319Z",
     "start_time": "2020-01-05T03:52:49.786134Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 400, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 373,301\n",
      "Trainable params: 373,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/6\n",
      "8982/8982 [==============================] - 863s 96ms/sample - loss: -28.1902 - accuracy: 0.0404 - val_loss: -58.6554 - val_accuracy: 0.0467\n",
      "Epoch 2/6\n",
      "8982/8982 [==============================] - 738s 82ms/sample - loss: -75.1898 - accuracy: 0.0481 - val_loss: -91.7079 - val_accuracy: 0.0467\n",
      "Epoch 3/6\n",
      "8982/8982 [==============================] - 723s 81ms/sample - loss: -105.7661 - accuracy: 0.0481 - val_loss: -121.4686 - val_accuracy: 0.0467\n",
      "Epoch 4/6\n",
      "8982/8982 [==============================] - 744s 83ms/sample - loss: -135.0146 - accuracy: 0.0481 - val_loss: -150.8156 - val_accuracy: 0.0467\n",
      "Epoch 5/6\n",
      "8982/8982 [==============================] - 858s 96ms/sample - loss: -164.2498 - accuracy: 0.0481 - val_loss: -179.6424 - val_accuracy: 0.0467\n",
      "Epoch 6/6\n",
      "8982/8982 [==============================] - 771s 86ms/sample - loss: -192.6920 - accuracy: 0.0481 - val_loss: -207.9033 - val_accuracy: 0.0467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xafb03488>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_2.summary())\n",
    "x_train_2, y_train_2 = x_train.copy(), y_train.copy()\n",
    "# update the model by epochs=6 and batch_size=138\n",
    "model_2.fit(x_train_2, y_train_2, validation_data=(x_test, y_test), epochs=6, batch_size=138)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T05:13:24.932154Z",
     "start_time": "2020-01-05T05:11:08.024323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 4.674978%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model_2.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.6f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T05:14:16.678113Z",
     "start_time": "2020-01-05T05:14:16.653112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-207.9033485881367, 0.046749778]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is a brief introduction to Recurrent Neural Network characteristics specific to a Long Short-Term Memory network. The scripting first imports the data prior to evaluation of its underlying numpy arrays. Then the dataset is prepared for ingestion into the required RNN.\n",
    "\n",
    "This entails decoding the imported numeric object arrays for identification of the encoded words. Once this is accomplished, the RNN-LSTM model is established with default hyperparameters.\n",
    "\n",
    "In total, 3 RNN-LSTM architectures were implimented in order to attempt model tuning. The models thus developed were the following:\n",
    "    - LSTM(units=100, activation='tanh')\n",
    "      (# basic default LSTM model)\n",
    "        * accuracy: 4.674978%\n",
    "        \n",
    "    - LSTM(units=100, activation='sigmoid', dropout=0.2)\n",
    "      (# update the model by updating `activation='sigmoid'` and adding `dropout=0.2`)\n",
    "        * accuracy: 4.674978%\n",
    "        \n",
    "    - LSTM(units=100, activation='sigmoid', dropout=0.2, use_bias=True, unit_forget_bias=True)\n",
    "      (# add new LSTM hyperparameter: ues_bias=True and unit_forget_bias=True)\n",
    "        * accuracy: 4.674978%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to the intended outcomes, these results are all equal and do not show any performance criteria improvements. The notion to utilize a `unit_forget_bias=True` is a recommendation identified within the KERAS website regarding the LSTM implimentation. In the development of the RNN here, defining this hyperparameter as `True`, was not influential.\n",
    "\n",
    "Other attempts to develop improved performance was the addition of `dropout=0.2`, that also did not impact the accuracy measure. Further, the hyperparameters `use_bias=True` had no effect, just like the other attempts.\n",
    "\n",
    "Equally, instantiating a `sigmoid` kernel as apposed to the `tanh` hyperparameter returned no differnce in the accuracy scores. However, the objectives of this assignment have been accomplished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "330px",
    "left": "1318.11px",
    "top": "51.77px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
